{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "247f4f0f",
   "metadata": {},
   "source": [
    "# üèÜ Insurance Fraud Detection Model Training - Step by Step Guide\n",
    "\n",
    "This notebook provides a comprehensive, educational approach to training a CNN model for insurance fraud detection. Each step is clearly explained with detailed comments so you can understand and modify the training process.\n",
    "\n",
    "## üìã What You'll Learn:\n",
    "1. **Dataset Analysis** - Understanding the fraud vs non-fraud data distribution\n",
    "2. **Data Preprocessing** - Image transformations and augmentation techniques  \n",
    "3. **Model Architecture** - EfficientNet-B1 with custom classifier\n",
    "4. **Loss Functions** - Focal Loss for handling class imbalance\n",
    "5. **Training Process** - Complete training loop with monitoring\n",
    "6. **Evaluation Metrics** - Precision, Recall, F1-Score analysis\n",
    "7. **Model Optimization** - Techniques for better performance\n",
    "\n",
    "## üéØ Training Goals:\n",
    "- **Target Precision**: 87-88% for fraud detection\n",
    "- **Target Recall**: 85%+ to catch most fraud cases\n",
    "- **Training Time**: 30-45 minutes on GPU\n",
    "- **Model Size**: Optimized for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b09e4b",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Import Required Libraries\n",
    "\n",
    "Let's start by importing all the necessary libraries for our fraud detection model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "\n",
    "# Pre-trained models library\n",
    "import timm\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Machine learning metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß All libraries imported successfully!\")\n",
    "print(\"üì± PyTorch version:\", torch.__version__)\n",
    "print(\"üéØ Ready to start fraud detection training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6292f",
   "metadata": {},
   "source": [
    "## üñ•Ô∏è Step 2: GPU Setup and Environment Configuration\n",
    "\n",
    "Let's check if we have GPU available and set up our computing environment for optimal training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c032fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device for training (GPU if available, CPU otherwise)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Clear GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()  # Python garbage collection\n",
    "    \n",
    "    print(\"üöÄ GPU AVAILABLE!\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(\"   ‚úÖ Perfect for fast training!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU NOT AVAILABLE - Using CPU\")\n",
    "    print(\"   Training will be slower but still functional\")\n",
    "    print(\"   Consider using Google Colab or Kaggle for GPU access\")\n",
    "\n",
    "print(f\"\\nüéØ Training device: {device}\")\n",
    "print(\"üîß Environment ready for fraud detection training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa816b",
   "metadata": {},
   "source": [
    "## üìÅ Step 3: Dataset Path Configuration\n",
    "\n",
    "Let's define the paths to our fraud detection dataset. The dataset is organized with separate folders for training and testing, each containing 'Fraud' and 'Non-Fraud' subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a58565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directory paths\n",
    "train_dir = r'../Insurance-Fraud-Detection/train'\n",
    "test_dir = r'../Insurance-Fraud-Detection/test'\n",
    "\n",
    "# Verify the dataset structure\n",
    "print(\"üìÅ Dataset Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if directories exist\n",
    "if os.path.exists(train_dir):\n",
    "    print(f\"‚úÖ Training directory found: {train_dir}\")\n",
    "    \n",
    "    # Count training samples\n",
    "    fraud_train_path = os.path.join(train_dir, 'Fraud')\n",
    "    non_fraud_train_path = os.path.join(train_dir, 'Non-Fraud')\n",
    "    \n",
    "    if os.path.exists(fraud_train_path):\n",
    "        fraud_count = len([f for f in os.listdir(fraud_train_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   üö® Fraud samples: {fraud_count:,}\")\n",
    "    \n",
    "    if os.path.exists(non_fraud_train_path):\n",
    "        non_fraud_count = len([f for f in os.listdir(non_fraud_train_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   ‚úÖ Non-fraud samples: {non_fraud_count:,}\")\n",
    "        \n",
    "        # Calculate class imbalance ratio\n",
    "        total_train = fraud_count + non_fraud_count\n",
    "        fraud_ratio = fraud_count / total_train * 100\n",
    "        print(f\"   üìä Fraud ratio: {fraud_ratio:.1f}%\")\n",
    "        print(f\"   ‚öñÔ∏è Class imbalance: {non_fraud_count/fraud_count:.1f}:1 (non-fraud:fraud)\")\n",
    "else:\n",
    "    print(\"‚ùå Training directory not found!\")\n",
    "\n",
    "if os.path.exists(test_dir):\n",
    "    print(f\"‚úÖ Test directory found: {test_dir}\")\n",
    "    \n",
    "    # Count test samples\n",
    "    fraud_test_path = os.path.join(test_dir, 'Fraud')\n",
    "    non_fraud_test_path = os.path.join(test_dir, 'Non-Fraud')\n",
    "    \n",
    "    if os.path.exists(fraud_test_path):\n",
    "        fraud_test_count = len([f for f in os.listdir(fraud_test_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   üö® Test fraud samples: {fraud_test_count:,}\")\n",
    "    \n",
    "    if os.path.exists(non_fraud_test_path):\n",
    "        non_fraud_test_count = len([f for f in os.listdir(non_fraud_test_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"   ‚úÖ Test non-fraud samples: {non_fraud_test_count:,}\")\n",
    "else:\n",
    "    print(\"‚ùå Test directory not found!\")\n",
    "\n",
    "print(\"\\nüéØ Dataset analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52247827",
   "metadata": {},
   "source": [
    "## üîç Step 4: Focal Loss Implementation\n",
    "\n",
    "**Why Focal Loss?** \n",
    "- Our dataset has severe class imbalance (25:1 ratio)\n",
    "- Standard cross-entropy loss gets overwhelmed by easy non-fraud examples\n",
    "- Focal Loss focuses training on hard-to-classify examples\n",
    "- Alpha parameter handles class imbalance\n",
    "- Gamma parameter focuses on hard examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61199446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    üéØ Focal Loss for handling class imbalance in fraud detection\n",
    "    \n",
    "    Formula: FL(p_t) = -Œ±_t * (1-p_t)^Œ≥ * log(p_t)\n",
    "    \n",
    "    Parameters:\n",
    "    - alpha: Balancing factor for rare class (fraud) vs common class (non-fraud)\n",
    "    - gamma: Focusing parameter - higher values focus more on hard examples\n",
    "    - reduction: How to aggregate the loss across batch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.75, gamma=5.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha      # 75% focus on fraud detection\n",
    "        self.gamma = gamma      # Focus on hard examples\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        print(f\"üéØ Focal Loss Configuration:\")\n",
    "        print(f\"   Œ± (fraud focus): {alpha:.2f} - Emphasizes fraud class\")\n",
    "        print(f\"   Œ≥ (hard examples): {gamma:.1f} - Focuses on difficult cases\")\n",
    "        print(f\"   üìà This helps with severe class imbalance!\")\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass of focal loss\n",
    "        \n",
    "        Args:\n",
    "            inputs: Model predictions (logits) [batch_size, num_classes]\n",
    "            targets: True labels [batch_size]\n",
    "        \n",
    "        Returns:\n",
    "            focal_loss: Computed focal loss value\n",
    "        \"\"\"\n",
    "        # Calculate standard cross-entropy loss\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=0.1)\n",
    "        \n",
    "        # Calculate probability of correct class\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # Clamp to avoid numerical instability\n",
    "        pt = torch.clamp(pt, min=1e-8, max=1-1e-8)\n",
    "        \n",
    "        # Calculate alpha term (class balancing)\n",
    "        # If target is 0 (fraud), use alpha; if 1 (non-fraud), use (1-alpha)\n",
    "        alpha_t = torch.where(targets == 0, self.alpha, 1 - self.alpha)\n",
    "        \n",
    "        # Calculate focal weight: (1-pt)^gamma\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        focal_weight = torch.clamp(focal_weight, min=1e-8, max=30.0)\n",
    "        \n",
    "        # Final focal loss: alpha * focal_weight * cross_entropy\n",
    "        focal_loss = alpha_t * focal_weight * ce_loss\n",
    "        \n",
    "        # Apply reduction\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Test the focal loss implementation\n",
    "print(\"üß™ Testing Focal Loss...\")\n",
    "focal_loss = FocalLoss(alpha=0.75, gamma=5.0)\n",
    "\n",
    "# Create dummy data for testing\n",
    "dummy_logits = torch.randn(4, 2)  # 4 samples, 2 classes\n",
    "dummy_targets = torch.tensor([0, 1, 0, 1])  # Mix of fraud and non-fraud\n",
    "\n",
    "test_loss = focal_loss(dummy_logits, dummy_targets)\n",
    "print(f\"‚úÖ Focal loss test successful! Loss value: {test_loss:.4f}\")\n",
    "print(\"üéØ Ready to use for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07773c9c",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Step 5: Model Architecture - EfficientNet-B1 Based Fraud Detector\n",
    "\n",
    "**Why EfficientNet-B1?**\n",
    "- Excellent balance between accuracy and speed\n",
    "- Pre-trained on ImageNet for strong feature extraction\n",
    "- Compound scaling for optimal performance\n",
    "- Perfect size for fraud detection (not too big, not too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947adfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDetectionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    üèóÔ∏è EfficientNet-B1 based fraud detection model\n",
    "    \n",
    "    Architecture:\n",
    "    1. EfficientNet-B1 backbone (pre-trained on ImageNet)\n",
    "    2. Custom classifier head with dropout and batch normalization\n",
    "    3. Optimized for fraud detection with bias initialization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=2):\n",
    "        super(FraudDetectionModel, self).__init__()\n",
    "        \n",
    "        print(\"üèóÔ∏è Building Fraud Detection Model...\")\n",
    "        \n",
    "        # Load pre-trained EfficientNet-B1 backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            'efficientnet_b1', \n",
    "            pretrained=True,    # Use ImageNet pre-trained weights\n",
    "            num_classes=0       # Remove the original classifier\n",
    "        )\n",
    "        \n",
    "        # Get the number of features from backbone\n",
    "        self.num_features = self.backbone.num_features\n",
    "        print(f\"   üìê Backbone features: {self.num_features}\")\n",
    "        \n",
    "        # Custom classifier head for fraud detection\n",
    "        self.classifier = nn.Sequential(\n",
    "            # First layer with high dropout to prevent overfitting\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(self.num_features, 256),\n",
    "            nn.BatchNorm1d(256),    # Batch normalization for stable training\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Second layer with moderate dropout\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Final classification layer\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize bias for better fraud detection\n",
    "        self._initialize_classifier_bias()\n",
    "        \n",
    "        print(f\"   üéØ Classifier input features: {self.num_features}\")\n",
    "        print(f\"   üéØ Output classes: {num_classes}\")\n",
    "        print(\"   ‚úÖ Model architecture ready!\")\n",
    "    \n",
    "    def _initialize_classifier_bias(self):\n",
    "        \"\"\"\n",
    "        üéØ Initialize classifier bias for better fraud detection\n",
    "        \n",
    "        This gives the model a slight preference toward detecting fraud,\n",
    "        which is important given the class imbalance.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Slight bias toward fraud detection (class 0)\n",
    "            self.classifier[-1].bias[0] = 1.0   # Fraud class\n",
    "            self.classifier[-1].bias[1] = -0.5  # Non-fraud class\n",
    "            \n",
    "        print(\"   üéØ Classifier bias initialized for fraud detection\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [batch_size, 3, 224, 224]\n",
    "            \n",
    "        Returns:\n",
    "            output: Class logits [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Extract features using EfficientNet backbone\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Classify using custom head\n",
    "        output = self.classifier(features)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        \"\"\"Get detailed information about the model\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'backbone_features': self.num_features,\n",
    "            'model_size_mb': total_params * 4 / (1024 ** 2)  # Approximate size in MB\n",
    "        }\n",
    "\n",
    "# Create and test the model\n",
    "print(\"üß™ Creating Fraud Detection Model...\")\n",
    "model = FraudDetectionModel(num_classes=2)\n",
    "\n",
    "# Get model information\n",
    "model_info = model.get_model_info()\n",
    "print(f\"\\nüìä Model Information:\")\n",
    "print(f\"   üìê Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"   üéØ Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "print(f\"   üíæ Approximate model size: {model_info['model_size_mb']:.1f} MB\")\n",
    "\n",
    "# Test model with dummy input\n",
    "print(f\"\\nüß™ Testing model with dummy input...\")\n",
    "dummy_input = torch.randn(2, 3, 224, 224)  # 2 images, 3 channels, 224x224\n",
    "dummy_output = model(dummy_input)\n",
    "print(f\"   ‚úÖ Input shape: {dummy_input.shape}\")\n",
    "print(f\"   ‚úÖ Output shape: {dummy_output.shape}\")\n",
    "print(f\"   ‚úÖ Model test successful!\")\n",
    "\n",
    "# Move model to device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "print(f\"üöÄ Model moved to {device}\")\n",
    "print(\"üéØ Fraud Detection Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905c7d4",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Step 6: Data Preprocessing and Augmentation\n",
    "\n",
    "**Why Data Augmentation?**\n",
    "- Increases dataset diversity without collecting new images\n",
    "- Helps model generalize better to unseen fraud cases\n",
    "- Reduces overfitting by creating variations\n",
    "- Simulates real-world image variations (rotation, lighting, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_transforms():\n",
    "    \"\"\"\n",
    "    üñºÔ∏è Create optimized data transformations for fraud detection\n",
    "    \n",
    "    Training transforms include augmentation for better generalization\n",
    "    Validation transforms are minimal for consistent evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training transforms with augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        # Resize and crop\n",
    "        transforms.Resize((240, 240)),      # Slightly larger for cropping\n",
    "        transforms.CenterCrop(224),         # EfficientNet-B1 input size\n",
    "        \n",
    "        # Augmentation techniques\n",
    "        transforms.RandomHorizontalFlip(p=0.5),     # 50% chance to flip\n",
    "        transforms.RandomRotation(degrees=10),       # Rotate ¬±10 degrees\n",
    "        transforms.ColorJitter(                      # Color variations\n",
    "            brightness=0.1,    # ¬±10% brightness\n",
    "            contrast=0.1,      # ¬±10% contrast\n",
    "            saturation=0.1,    # ¬±10% saturation\n",
    "            hue=0.05          # ¬±5% hue\n",
    "        ),\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "            std=[0.229, 0.224, 0.225]   # ImageNet std\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Validation transforms (no augmentation)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((240, 240)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    print(\"üñºÔ∏è Data Transforms Created:\")\n",
    "    print(\"   üìà Training: Resize ‚Üí Crop ‚Üí Augment ‚Üí Normalize\")\n",
    "    print(\"   üìä Validation: Resize ‚Üí Crop ‚Üí Normalize\")\n",
    "    print(\"   üéØ Input size: 224x224 (optimized for EfficientNet)\")\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# Create the transforms\n",
    "train_transform, val_transform = create_data_transforms()\n",
    "\n",
    "# Test transforms with a dummy image\n",
    "print(\"\\nüß™ Testing transforms...\")\n",
    "dummy_pil_image = Image.new('RGB', (300, 300), color='red')\n",
    "\n",
    "# Test training transform\n",
    "train_tensor = train_transform(dummy_pil_image)\n",
    "print(f\"   ‚úÖ Training transform output: {train_tensor.shape}\")\n",
    "print(f\"   üìä Tensor range: [{train_tensor.min():.3f}, {train_tensor.max():.3f}]\")\n",
    "\n",
    "# Test validation transform  \n",
    "val_tensor = val_transform(dummy_pil_image)\n",
    "print(f\"   ‚úÖ Validation transform output: {val_tensor.shape}\")\n",
    "print(\"   üéØ Transforms ready for dataset creation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e678acc",
   "metadata": {},
   "source": [
    "## üìä Step 7: Custom Dataset Class with Smart Balancing\n",
    "\n",
    "**Key Features:**\n",
    "- Handles severe class imbalance (200 fraud vs 5000 non-fraud)\n",
    "- Intelligent sampling to maintain fraud detection performance\n",
    "- Configurable fraud ratio for optimal training\n",
    "- Memory-efficient loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FraudDataset(Dataset):\n",
    "    \"\"\"\n",
    "    üìä Custom dataset for fraud detection with intelligent class balancing\n",
    "    \n",
    "    Features:\n",
    "    - Loads all fraud samples (never lose fraud data)\n",
    "    - Smart non-fraud sampling for optimal training balance\n",
    "    - Configurable fraud ratio\n",
    "    - Memory-efficient image loading\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, fraud_ratio=0.3, max_samples=None):\n",
    "        \"\"\"\n",
    "        Initialize the fraud detection dataset\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to data directory (contains 'Fraud' and 'Non-Fraud' folders)\n",
    "            transform: Data transformations to apply\n",
    "            fraud_ratio: Target ratio of fraud samples (0.3 = 30% fraud)\n",
    "            max_samples: Maximum total samples to load (for memory management)\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.labels = []\n",
    "        \n",
    "        print(f\"üìÇ Loading dataset from: {data_dir}\")\n",
    "        print(f\"üéØ Target fraud ratio: {fraud_ratio:.1%}\")\n",
    "        \n",
    "        # Define class paths\n",
    "        fraud_dir = os.path.join(data_dir, 'Fraud')\n",
    "        non_fraud_dir = os.path.join(data_dir, 'Non-Fraud')\n",
    "        \n",
    "        # Load ALL fraud samples (critical - never lose fraud data!)\n",
    "        fraud_files = [f for f in os.listdir(fraud_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        for img_name in fraud_files:\n",
    "            self.image_files.append(os.path.join(fraud_dir, img_name))\n",
    "            self.labels.append(0)  # Fraud = 0\n",
    "        \n",
    "        fraud_count = len(fraud_files)\n",
    "        print(f\"   üö® Loaded fraud samples: {fraud_count}\")\n",
    "        \n",
    "        # Calculate target non-fraud count based on desired ratio\n",
    "        # fraud_ratio = fraud_count / (fraud_count + non_fraud_count)\n",
    "        # Solving for non_fraud_count:\n",
    "        target_non_fraud = int(fraud_count * (1 - fraud_ratio) / fraud_ratio)\n",
    "        \n",
    "        # Apply max_samples limit if specified\n",
    "        if max_samples:\n",
    "            available_for_non_fraud = max_samples - fraud_count\n",
    "            target_non_fraud = min(target_non_fraud, available_for_non_fraud)\n",
    "        \n",
    "        print(f\"   üéØ Target non-fraud samples: {target_non_fraud}\")\n",
    "        \n",
    "        # Load non-fraud samples up to target count\n",
    "        non_fraud_files = [f for f in os.listdir(non_fraud_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        # Shuffle for random sampling\n",
    "        import random\n",
    "        random.shuffle(non_fraud_files)\n",
    "        \n",
    "        loaded_non_fraud = 0\n",
    "        for img_name in non_fraud_files:\n",
    "            if loaded_non_fraud >= target_non_fraud:\n",
    "                break\n",
    "                \n",
    "            self.image_files.append(os.path.join(non_fraud_dir, img_name))\n",
    "            self.labels.append(1)  # Non-fraud = 1\n",
    "            loaded_non_fraud += 1\n",
    "        \n",
    "        # Calculate actual statistics\n",
    "        total_samples = len(self.labels)\n",
    "        actual_fraud_ratio = fraud_count / total_samples\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded non-fraud samples: {loaded_non_fraud}\")\n",
    "        print(f\"   üìä Total samples: {total_samples}\")\n",
    "        print(f\"   üìà Actual fraud ratio: {actual_fraud_ratio:.1%}\")\n",
    "        print(f\"   ‚öñÔ∏è Balance ratio: {loaded_non_fraud/fraud_count:.1f}:1 (non-fraud:fraud)\")\n",
    "        \n",
    "        # Calculate class distribution\n",
    "        self.class_counts = Counter(self.labels)\n",
    "        print(f\"   üìã Class distribution: {dict(self.class_counts)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of samples\"\"\"\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index\n",
    "            \n",
    "        Returns:\n",
    "            image: Transformed image tensor\n",
    "            label: Class label (0=fraud, 1=non-fraud)\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        img_path = self.image_files[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"\n",
    "        Calculate class weights for loss function balancing\n",
    "        \n",
    "        Returns:\n",
    "            weights: List of weights for each class [fraud_weight, non_fraud_weight]\n",
    "        \"\"\"\n",
    "        fraud_count = self.class_counts[0]\n",
    "        non_fraud_count = self.class_counts[1]\n",
    "        total_count = fraud_count + non_fraud_count\n",
    "        \n",
    "        # Inverse frequency weighting\n",
    "        fraud_weight = total_count / (2 * fraud_count)\n",
    "        non_fraud_weight = total_count / (2 * non_fraud_count)\n",
    "        \n",
    "        return [fraud_weight, non_fraud_weight]\n",
    "    \n",
    "    def visualize_samples(self, num_samples=4):\n",
    "        \"\"\"\n",
    "        Visualize random samples from the dataset\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to show\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, num_samples//2, figsize=(12, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Get random indices\n",
    "        indices = random.sample(range(len(self)), num_samples)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            image, label = self[idx]\n",
    "            \n",
    "            # Convert tensor back to PIL for visualization\n",
    "            if isinstance(image, torch.Tensor):\n",
    "                # Denormalize\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                image = image * std + mean\n",
    "                image = torch.clamp(image, 0, 1)\n",
    "                image = transforms.ToPILImage()(image)\n",
    "            \n",
    "            axes[i].imshow(image)\n",
    "            axes[i].set_title(f\"{'üö® Fraud' if label == 0 else '‚úÖ Non-Fraud'}\")\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test the dataset\n",
    "print(\"üß™ Testing FraudDataset...\")\n",
    "test_dataset = FraudDataset(\n",
    "    data_dir=train_dir,\n",
    "    transform=train_transform,\n",
    "    fraud_ratio=0.3,\n",
    "    max_samples=1000  # Small sample for testing\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset Test Results:\")\n",
    "print(f\"   üìè Dataset length: {len(test_dataset)}\")\n",
    "print(f\"   ‚öñÔ∏è Class weights: {test_dataset.get_class_weights()}\")\n",
    "\n",
    "# Test loading a sample\n",
    "sample_image, sample_label = test_dataset[0]\n",
    "print(f\"   üñºÔ∏è Sample image shape: {sample_image.shape}\")\n",
    "print(f\"   üè∑Ô∏è Sample label: {sample_label} ({'Fraud' if sample_label == 0 else 'Non-Fraud'})\")\n",
    "print(\"   ‚úÖ Dataset test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef73d8",
   "metadata": {},
   "source": [
    "## üîÑ Step 8: Create Data Loaders with Advanced Sampling\n",
    "\n",
    "**WeightedRandomSampler:**\n",
    "- Ensures balanced training despite class imbalance\n",
    "- Gives fraud samples higher probability of being selected\n",
    "- Creates effective oversampling without data duplication\n",
    "- Improves fraud detection recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(batch_size=16, fraud_ratio=0.3, num_workers=0):\n",
    "    \"\"\"\n",
    "    üîÑ Create optimized data loaders for fraud detection training\n",
    "    \n",
    "    Args:\n",
    "        batch_size: Number of samples per batch\n",
    "        fraud_ratio: Target fraud ratio in dataset\n",
    "        num_workers: Number of worker processes for data loading\n",
    "        \n",
    "    Returns:\n",
    "        train_loader: Training data loader with weighted sampling\n",
    "        test_loader: Test data loader for evaluation\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üîÑ Creating Data Loaders...\")\n",
    "    print(f\"   üì¶ Batch size: {batch_size}\")\n",
    "    print(f\"   üéØ Target fraud ratio: {fraud_ratio:.1%}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FraudDataset(\n",
    "        data_dir=train_dir,\n",
    "        transform=train_transform,\n",
    "        fraud_ratio=fraud_ratio\n",
    "    )\n",
    "    \n",
    "    test_dataset = FraudDataset(\n",
    "        data_dir=test_dir,\n",
    "        transform=val_transform,\n",
    "        fraud_ratio=0.5,  # Keep natural test distribution\n",
    "        max_samples=2000  # Limit test size for faster evaluation\n",
    "    )\n",
    "    \n",
    "    # Calculate sample weights for training\n",
    "    print(f\"\\n‚öñÔ∏è Calculating sample weights for balanced training...\")\n",
    "    \n",
    "    # Get class weights\n",
    "    class_weights = train_dataset.get_class_weights()\n",
    "    fraud_weight = class_weights[0] * 2.0  # Extra emphasis on fraud\n",
    "    non_fraud_weight = class_weights[1]\n",
    "    \n",
    "    print(f\"   üö® Fraud weight: {fraud_weight:.2f}\")\n",
    "    print(f\"   ‚úÖ Non-fraud weight: {non_fraud_weight:.2f}\")\n",
    "    \n",
    "    # Create sample weights list\n",
    "    sample_weights = []\n",
    "    for label in train_dataset.labels:\n",
    "        if label == 0:  # Fraud\n",
    "            sample_weights.append(fraud_weight)\n",
    "        else:  # Non-fraud\n",
    "            sample_weights.append(non_fraud_weight)\n",
    "    \n",
    "    # Create weighted sampler for balanced training\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights) * 2,  # 2x oversampling\n",
    "        replacement=True  # Allow sampling with replacement\n",
    "    )\n",
    "    \n",
    "    print(f\"   üîÑ Sampler created with {len(sample_weights) * 2:,} samples per epoch\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,  # Use weighted sampler\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        drop_last=True  # Drop incomplete batches for stable training\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # Keep consistent test order\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if device.type == 'cuda' else False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Data Loader Statistics:\")\n",
    "    print(f\"   üöÇ Training batches per epoch: {len(train_loader):,}\")\n",
    "    print(f\"   üß™ Test batches: {len(test_loader):,}\")\n",
    "    print(f\"   üì¶ Samples per training batch: {batch_size}\")\n",
    "    print(f\"   üîÑ Total training samples per epoch: {len(train_loader) * batch_size:,}\")\n",
    "    \n",
    "    return train_loader, test_loader, train_dataset, test_dataset\n",
    "\n",
    "# Create the data loaders\n",
    "print(\"üöÄ Creating optimized data loaders...\")\n",
    "train_loader, test_loader, train_dataset, test_dataset = create_data_loaders(\n",
    "    batch_size=12,  # Optimized for GPU memory\n",
    "    fraud_ratio=0.3,\n",
    "    num_workers=0  # Set to 0 for Windows compatibility\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data loaders created successfully!\")\n",
    "print(\"üéØ Ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1694d0",
   "metadata": {},
   "source": [
    "## üìà Step 9: Training Setup and Optimization\n",
    "\n",
    "**Optimizer Strategy:**\n",
    "- **AdamW**: Better weight decay than standard Adam\n",
    "- **Differential Learning Rates**: Lower LR for pre-trained backbone, higher for classifier\n",
    "- **ReduceLROnPlateau**: Automatically reduce learning rate when stuck\n",
    "- **Gradient Clipping**: Prevents gradient explosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss function\n",
    "criterion = FocalLoss(alpha=0.75, gamma=5.0)\n",
    "\n",
    "# Setup optimizer with differential learning rates\n",
    "optimizer = optim.AdamW([\n",
    "    # Lower learning rate for pre-trained backbone\n",
    "    {'params': model.backbone.parameters(), 'lr': 2e-5, 'weight_decay': 0.01},\n",
    "    # Higher learning rate for new classifier\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3, 'weight_decay': 0.01}\n",
    "])\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',          # Monitor for maximum metric (like F1-score)\n",
    "    factor=0.7,          # Reduce LR by 30% when plateau\n",
    "    patience=2,          # Wait 2 epochs before reducing\n",
    "    min_lr=1e-7,         # Minimum learning rate\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"üìà Training Setup Complete:\")\n",
    "print(f\"   üéØ Loss Function: Focal Loss (Œ±=0.75, Œ≥=5.0)\")\n",
    "print(f\"   üîß Optimizer: AdamW with differential learning rates\")\n",
    "print(f\"   üìä Backbone LR: 2e-5 (fine-tuning)\")\n",
    "print(f\"   üìä Classifier LR: 1e-3 (learning from scratch)\")\n",
    "print(f\"   üìâ Scheduler: ReduceLROnPlateau (patience=2)\")\n",
    "print(\"   ‚úÖ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b0352",
   "metadata": {},
   "source": [
    "## üìä Step 10: Evaluation Metrics Functions\n",
    "\n",
    "Let's create comprehensive evaluation functions to monitor our model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bb950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, verbose=True):\n",
    "    \"\"\"\n",
    "    üìä Calculate comprehensive metrics for fraud detection\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        verbose: Whether to print detailed results\n",
    "        \n",
    "    Returns:\n",
    "        metrics: Dictionary containing all calculated metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision, Recall, F1 for each class\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Extract fraud-specific metrics (class 0)\n",
    "    fraud_precision = precision[0] if len(precision) > 0 else 0.0\n",
    "    fraud_recall = recall[0] if len(recall) > 0 else 0.0\n",
    "    fraud_f1 = f1[0] if len(f1) > 0 else 0.0\n",
    "    \n",
    "    # Extract non-fraud metrics (class 1)\n",
    "    non_fraud_precision = precision[1] if len(precision) > 1 else 0.0\n",
    "    non_fraud_recall = recall[1] if len(recall) > 1 else 0.0\n",
    "    non_fraud_f1 = f1[1] if len(f1) > 1 else 0.0\n",
    "    \n",
    "    # Custom balanced score (emphasizes fraud detection)\n",
    "    balance_score = (0.6 * fraud_precision + 0.4 * fraud_recall)\n",
    "    \n",
    "    # Create metrics dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'fraud_precision': fraud_precision,\n",
    "        'fraud_recall': fraud_recall,\n",
    "        'fraud_f1': fraud_f1,\n",
    "        'non_fraud_precision': non_fraud_precision,\n",
    "        'non_fraud_recall': non_fraud_recall,\n",
    "        'non_fraud_f1': non_fraud_f1,\n",
    "        'balance_score': balance_score,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Evaluation Metrics:\")\n",
    "        print(f\"   üéØ Overall Accuracy: {accuracy*100:.1f}%\")\n",
    "        print(f\"   üö® Fraud Precision: {fraud_precision*100:.1f}%\")\n",
    "        print(f\"   üö® Fraud Recall: {fraud_recall*100:.1f}%\")\n",
    "        print(f\"   üö® Fraud F1-Score: {fraud_f1*100:.1f}%\")\n",
    "        print(f\"   ‚úÖ Non-Fraud Precision: {non_fraud_precision*100:.1f}%\")\n",
    "        print(f\"   ‚úÖ Non-Fraud Recall: {non_fraud_recall*100:.1f}%\")\n",
    "        print(f\"   ‚öñÔ∏è Balance Score: {balance_score*100:.1f}%\")\n",
    "        print(f\"   üìã Confusion Matrix:\")\n",
    "        print(f\"      Predicted:  [Fraud] [Non-Fraud]\")\n",
    "        print(f\"      Fraud:      [{cm[0,0]:4d}]   [{cm[0,1]:4d}]\")\n",
    "        print(f\"      Non-Fraud:  [{cm[1,0]:4d}]   [{cm[1,1]:4d}]\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    üìä Plot confusion matrix with nice visualization\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fraud', 'Non-Fraud'],\n",
    "                yticklabels=['Fraud', 'Non-Fraud'])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    üìà Plot training history with multiple metrics\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0,0].plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
    "    axes[0,0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss')\n",
    "    axes[0,0].set_title('Training and Validation Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0,1].plot(epochs, history['accuracy'], 'g-', label='Accuracy')\n",
    "    axes[0,1].set_title('Model Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # Fraud metrics plot\n",
    "    axes[1,0].plot(epochs, history['fraud_precision'], 'b-', label='Precision')\n",
    "    axes[1,0].plot(epochs, history['fraud_recall'], 'r-', label='Recall')\n",
    "    axes[1,0].plot(epochs, history['fraud_f1'], 'g-', label='F1-Score')\n",
    "    axes[1,0].set_title('Fraud Detection Metrics')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Score')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Balance score plot\n",
    "    axes[1,1].plot(epochs, history['balance_score'], 'purple', label='Balance Score')\n",
    "    axes[1,1].set_title('Balance Score (60% Precision + 40% Recall)')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Score')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìä Evaluation functions created:\")\n",
    "print(\"   üéØ calculate_metrics() - Comprehensive metric calculation\")\n",
    "print(\"   üìã plot_confusion_matrix() - Visual confusion matrix\")\n",
    "print(\"   üìà plot_training_history() - Training progress visualization\")\n",
    "print(\"   ‚úÖ Ready for training monitoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133296b",
   "metadata": {},
   "source": [
    "## üöÄ Step 11: Complete Training Loop\n",
    "\n",
    "This is the heart of our fraud detection training - a comprehensive training loop with real-time monitoring and automatic best model saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NUM_EPOCHS = 15\n",
    "TARGET_PRECISION = 87.0  # Target 87% precision\n",
    "TARGET_RECALL = 85.0     # Target 85% recall\n",
    "\n",
    "# Initialize training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'accuracy': [],\n",
    "    'fraud_precision': [],\n",
    "    'fraud_recall': [],\n",
    "    'fraud_f1': [],\n",
    "    'balance_score': []\n",
    "}\n",
    "\n",
    "# Best model tracking\n",
    "best_balance_score = 0.0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"üöÄ Starting Fraud Detection Training!\")\n",
    "print(f\"üéØ Target: {TARGET_PRECISION}% Precision, {TARGET_RECALL}% Recall\")\n",
    "print(f\"‚è±Ô∏è Maximum epochs: {NUM_EPOCHS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüîÑ EPOCH {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # TRAINING PHASE\n",
    "    # =============================================================================\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    print(\"üìö Training Phase...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Progress update every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_acc = 100.0 * train_correct / train_total\n",
    "            current_loss = train_loss / (batch_idx + 1)\n",
    "            progress = (batch_idx + 1) / len(train_loader) * 100\n",
    "            print(f\"   Batch {batch_idx+1:4d}/{len(train_loader)} ({progress:5.1f}%) | \"\n",
    "                  f\"Loss: {current_loss:.4f} | Acc: {current_acc:.1f}%\")\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100.0 * train_correct / train_total\n",
    "    \n",
    "    # =============================================================================\n",
    "    # VALIDATION PHASE\n",
    "    # =============================================================================\n",
    "    print(\"üß™ Validation Phase...\")\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss / len(test_loader)\n",
    "    metrics = calculate_metrics(all_labels, all_predictions, verbose=False)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    accuracy = metrics['accuracy'] * 100\n",
    "    fraud_precision = metrics['fraud_precision'] * 100\n",
    "    fraud_recall = metrics['fraud_recall'] * 100\n",
    "    fraud_f1 = metrics['fraud_f1'] * 100\n",
    "    balance_score = metrics['balance_score'] * 100\n",
    "    \n",
    "    # =============================================================================\n",
    "    # EPOCH SUMMARY\n",
    "    # =============================================================================\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    total_time = time.time() - training_start_time\n",
    "    \n",
    "    print(f\"\\nüìä EPOCH {epoch+1} RESULTS:\")\n",
    "    print(f\"   ‚è±Ô∏è Time: {epoch_time:.1f}s | Total: {total_time/60:.1f}min\")\n",
    "    print(f\"   üìâ Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"   üéØ Accuracy: {accuracy:.1f}%\")\n",
    "    print(f\"   üö® Fraud Precision: {fraud_precision:.1f}%\")\n",
    "    print(f\"   üö® Fraud Recall: {fraud_recall:.1f}%\")\n",
    "    print(f\"   üö® Fraud F1-Score: {fraud_f1:.1f}%\")\n",
    "    print(f\"   ‚öñÔ∏è Balance Score: {balance_score:.1f}%\")\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    history['accuracy'].append(accuracy)\n",
    "    history['fraud_precision'].append(fraud_precision)\n",
    "    history['fraud_recall'].append(fraud_recall)\n",
    "    history['fraud_f1'].append(fraud_f1)\n",
    "    history['balance_score'].append(balance_score)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # MODEL SAVING AND EARLY STOPPING\n",
    "    # =============================================================================\n",
    "    \n",
    "    # Save best model\n",
    "    if balance_score > best_balance_score and fraud_precision >= 75:\n",
    "        best_balance_score = balance_score\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save(model.state_dict(), 'best_fraud_model.pth')\n",
    "        patience_counter = 0\n",
    "        print(f\"   ‚úÖ NEW BEST MODEL! Balance Score: {balance_score:.1f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Check if target achieved\n",
    "    if fraud_precision >= TARGET_PRECISION and fraud_recall >= TARGET_RECALL:\n",
    "        print(f\"\\nüéâ TARGET ACHIEVED!\")\n",
    "        print(f\"   üéØ Precision: {fraud_precision:.1f}% (‚â•{TARGET_PRECISION}%)\")\n",
    "        print(f\"   üö® Recall: {fraud_recall:.1f}% (‚â•{TARGET_RECALL}%)\")\n",
    "        print(f\"   ‚è±Ô∏è Time to target: {total_time/60:.1f} minutes\")\n",
    "        break\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(balance_score)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if patience_counter >= 5:\n",
    "        print(f\"\\n‚è∏Ô∏è Early stopping triggered (patience=5)\")\n",
    "        break\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING COMPLETE\n",
    "# =============================================================================\n",
    "total_training_time = time.time() - training_start_time\n",
    "\n",
    "print(f\"\\nüèÅ TRAINING COMPLETE!\")\n",
    "print(f\"‚è±Ô∏è Total training time: {total_training_time/60:.1f} minutes\")\n",
    "print(f\"üèÜ Best balance score: {best_balance_score:.1f}%\")\n",
    "print(f\"üìà Training epochs completed: {len(history['train_loss'])}\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"‚úÖ Best model loaded for final evaluation\")\n",
    "\n",
    "print(\"üéØ Training phase complete! Ready for final evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f771d65a",
   "metadata": {},
   "source": [
    "## üìä Step 12: Final Evaluation and Visualization\n",
    "\n",
    "Let's evaluate our trained model and visualize the training progress and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive evaluation\n",
    "print(\"üß™ Performing Final Comprehensive Evaluation...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "final_predictions = []\n",
    "final_labels = []\n",
    "final_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        final_predictions.extend(predicted.cpu().numpy())\n",
    "        final_labels.extend(labels.cpu().numpy())\n",
    "        final_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "final_metrics = calculate_metrics(final_labels, final_predictions, verbose=True)\n",
    "\n",
    "print(f\"\\nüèÜ FINAL MODEL PERFORMANCE:\")\n",
    "print(f\"   üéØ Overall Accuracy: {final_metrics['accuracy']*100:.1f}%\")\n",
    "print(f\"   üö® Fraud Precision: {final_metrics['fraud_precision']*100:.1f}%\")\n",
    "print(f\"   üö® Fraud Recall: {final_metrics['fraud_recall']*100:.1f}%\")\n",
    "print(f\"   üö® Fraud F1-Score: {final_metrics['fraud_f1']*100:.1f}%\")\n",
    "print(f\"   ‚öñÔ∏è Balance Score: {final_metrics['balance_score']*100:.1f}%\")\n",
    "\n",
    "# Success evaluation\n",
    "fraud_precision_final = final_metrics['fraud_precision'] * 100\n",
    "fraud_recall_final = final_metrics['fraud_recall'] * 100\n",
    "\n",
    "if fraud_precision_final >= 87 and fraud_recall_final >= 85:\n",
    "    print(\"\\nüéâ TARGET ACHIEVED! Model ready for deployment!\")\n",
    "    success_level = \"EXCELLENT\"\n",
    "elif fraud_precision_final >= 85 and fraud_recall_final >= 80:\n",
    "    print(\"\\n‚úÖ VERY GOOD performance! Close to target.\")\n",
    "    success_level = \"VERY GOOD\"\n",
    "elif fraud_precision_final >= 80:\n",
    "    print(\"\\nüìà GOOD performance! Consider additional training.\")\n",
    "    success_level = \"GOOD\"\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Performance below target. Review hyperparameters.\")\n",
    "    success_level = \"NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"üèÖ Performance Level: {success_level}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "print(\"\\nüìà Plotting Training History...\")\n",
    "plot_training_history(history)\n",
    "\n",
    "# Plot confusion matrix\n",
    "print(\"\\nüìã Plotting Final Confusion Matrix...\")\n",
    "plot_confusion_matrix(final_metrics['confusion_matrix'], \n",
    "                     title=\"Final Model Confusion Matrix\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìä Detailed Classification Report:\")\n",
    "print(classification_report(final_labels, final_predictions, \n",
    "                          target_names=['Fraud', 'Non-Fraud'], digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42061108",
   "metadata": {},
   "source": [
    "## üíæ Step 13: Model Saving and Export\n",
    "\n",
    "Save the trained model for deployment and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete model\n",
    "print(\"üíæ Saving trained model...\")\n",
    "\n",
    "# Save the complete model (architecture + weights)\n",
    "torch.save(model, 'final_fraud_detection_model.pth')\n",
    "print(\"‚úÖ Complete model saved as 'final_fraud_detection_model.pth'\")\n",
    "\n",
    "# Save only the state dict (more efficient)\n",
    "torch.save(model.state_dict(), 'final_fraud_model_weights.pth')\n",
    "print(\"‚úÖ Model weights saved as 'final_fraud_model_weights.pth'\")\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "with open('training_history.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    history_for_json = {}\n",
    "    for key, value in history.items():\n",
    "        if isinstance(value[0], np.ndarray):\n",
    "            history_for_json[key] = [float(v) for v in value]\n",
    "        else:\n",
    "            history_for_json[key] = value\n",
    "    json.dump(history_for_json, f, indent=2)\n",
    "print(\"‚úÖ Training history saved as 'training_history.json'\")\n",
    "\n",
    "# Save model info\n",
    "model_info = {\n",
    "    'model_name': 'EfficientNet-B1 Fraud Detector',\n",
    "    'input_size': [3, 224, 224],\n",
    "    'num_classes': 2,\n",
    "    'final_performance': {\n",
    "        'accuracy': float(final_metrics['accuracy']),\n",
    "        'fraud_precision': float(final_metrics['fraud_precision']),\n",
    "        'fraud_recall': float(final_metrics['fraud_recall']),\n",
    "        'fraud_f1': float(final_metrics['fraud_f1']),\n",
    "        'balance_score': float(final_metrics['balance_score'])\n",
    "    },\n",
    "    'training_epochs': len(history['train_loss']),\n",
    "    'training_time_minutes': float(total_training_time / 60),\n",
    "    'target_achieved': fraud_precision_final >= 87 and fraud_recall_final >= 85\n",
    "}\n",
    "\n",
    "with open('model_info.json', 'w') as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "print(\"‚úÖ Model info saved as 'model_info.json'\")\n",
    "\n",
    "print(f\"\\nüìÅ Files saved:\")\n",
    "print(f\"   üß† final_fraud_detection_model.pth - Complete model\")\n",
    "print(f\"   ‚öñÔ∏è final_fraud_model_weights.pth - Model weights only\")\n",
    "print(f\"   üìà training_history.json - Training progress data\")\n",
    "print(f\"   üìä model_info.json - Model metadata and performance\")\n",
    "\n",
    "print(f\"\\nüéØ Model Summary:\")\n",
    "print(f\"   üèóÔ∏è Architecture: EfficientNet-B1 + Custom Classifier\")\n",
    "print(f\"   üìê Parameters: {model_info['final_performance']}\")\n",
    "print(f\"   ‚è±Ô∏è Training Time: {model_info['training_time_minutes']:.1f} minutes\")\n",
    "print(f\"   üéØ Target Achieved: {'‚úÖ YES' if model_info['target_achieved'] else '‚ùå NO'}\")\n",
    "\n",
    "print(\"\\nüéâ Model training and saving complete!\")\n",
    "print(\"üöÄ Your fraud detection model is ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
