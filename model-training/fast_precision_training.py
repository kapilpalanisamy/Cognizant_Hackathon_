# âš¡ FAST PRECISION FRAUD DETECTION - Optimized for Speed & Performance
# Target: 87-88% precision with 85%+ recall in 30-45 minutes
# Strategy: Optimized architecture + smart training

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler
import timm
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
from collections import Counter
import json
import gc
import time
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

print("âš¡ FAST PRECISION FRAUD DETECTION TRAINING")
print("Target: 87-88% precision with 85%+ recall in 30-45 minutes")
print("Strategy: Optimized architecture + smart parameters")
print("=" * 70)

# GPU Setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    gc.collect()
    print(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
else:
    print("âŒ No GPU - training will be slow!")

# Dataset paths
train_dir = r'Insurance-Fraud-Detection\train'
test_dir = r'Insurance-Fraud-Detection\test'

class FastPrecisionFocalLoss(nn.Module):
    """
    âš¡ Fast-precision focal loss - optimized for speed and performance
    """
    
    def __init__(self, alpha=0.75, gamma=5.0, reduction='mean'):
        super(FastPrecisionFocalLoss, self).__init__()
        self.alpha = alpha  # 75% focus on fraud (optimal balance)
        self.gamma = gamma  # Moderate focusing for speed
        self.reduction = reduction
        
        print(f"âš¡ Fast-Precision Focal Loss:")
        print(f"   Î± (fraud focus): {alpha:.2f} (optimal balance)")
        print(f"   Î³ (hard examples): {gamma:.1f} (speed-optimized)")
        print("   ğŸ¯ Target: 87-88% precision, fast convergence")
    
    def forward(self, inputs, targets):
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', label_smoothing=0.1)
        pt = torch.exp(-ce_loss)
        pt = torch.clamp(pt, min=1e-8, max=1-1e-8)
        
        alpha_t = torch.where(targets == 0, self.alpha, 1 - self.alpha)
        focal_weight = (1 - pt) ** self.gamma
        focal_weight = torch.clamp(focal_weight, min=1e-8, max=30.0)
        
        focal_loss = alpha_t * focal_weight * ce_loss
        
        return focal_loss.mean() if self.reduction == 'mean' else focal_loss

class FastPrecisionDetector(nn.Module):
    """
    âš¡ Fast precision detector - optimized EfficientNet-B1 for speed
    """
    
    def __init__(self, num_classes=2):
        super(FastPrecisionDetector, self).__init__()
        
        # EfficientNet-B1 - good balance of speed and accuracy
        self.backbone = timm.create_model('efficientnet_b1', pretrained=True, num_classes=0)
        self.num_features = self.backbone.num_features
        
        # Streamlined classifier for speed
        self.classifier = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(self.num_features, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )
        
        # Precision-optimized bias initialization
        with torch.no_grad():
            self.classifier[-1].bias[0] = 1.0   # Moderate bias toward fraud
            self.classifier[-1].bias[1] = -0.5  # Moderate bias against non-fraud
    
    def forward(self, x):
        features = self.backbone(x)
        output = self.classifier(features)
        return output

class OptimizedDataset(Dataset):
    def __init__(self, data_dir, transform=None, fraud_ratio=0.3, precision_focus=True):
        self.data_dir = data_dir
        self.transform = transform
        self.image_files = []
        self.labels = []
        
        # Load data with optimized ratio for fast precision learning
        fraud_dir = os.path.join(data_dir, 'Fraud')
        non_fraud_dir = os.path.join(data_dir, 'Non-Fraud')
        
        # Load ALL fraud samples (never lose fraud data)
        fraud_files = os.listdir(fraud_dir)
        for img_name in fraud_files:
            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                self.image_files.append(os.path.join(fraud_dir, img_name))
                self.labels.append(0)  # Fraud = 0
        
        fraud_count = len([l for l in self.labels if l == 0])
        
        # Optimized non-fraud ratio for precision (not too extreme)
        if precision_focus:
            target_non_fraud = int(fraud_count * (1 - fraud_ratio) / fraud_ratio)
            target_non_fraud = min(target_non_fraud, 1000)  # Reasonable limit
        else:
            target_non_fraud = int(fraud_count * 2)
        
        # Load optimized non-fraud samples
        non_fraud_files = os.listdir(non_fraud_dir)
        loaded_non_fraud = 0
        for img_name in non_fraud_files:
            if loaded_non_fraud >= target_non_fraud:
                break
            if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):
                self.image_files.append(os.path.join(non_fraud_dir, img_name))
                self.labels.append(1)  # Non-fraud = 1
                loaded_non_fraud += 1
        
        print(f"âš¡ Optimized Dataset (target fraud ratio: {fraud_ratio:.1%}):")
        print(f"   Fraud samples: {sum(1 for l in self.labels if l == 0)}")
        print(f"   Non-fraud samples: {sum(1 for l in self.labels if l == 1)}")
        actual_ratio = fraud_count / len(self.labels)
        print(f"   Actual fraud ratio: {actual_ratio:.1%}")
        print(f"   âš¡ Optimized for fast precision learning")

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

def get_fast_transforms():
    """Fast transforms optimized for speed"""
    train_transform = transforms.Compose([
        transforms.Resize((240, 240)),  # Optimized size for EfficientNet-B1
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=0.1, contrast=0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((240, 240)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def create_fast_loaders():
    """Create fast, optimized data loaders"""
    train_transform, val_transform = get_fast_transforms()
    
    # Optimized ratios for fast precision learning
    train_dataset = OptimizedDataset(train_dir, transform=train_transform, fraud_ratio=0.3, precision_focus=True)
    test_dataset = OptimizedDataset(test_dir, transform=val_transform, fraud_ratio=0.3, precision_focus=False)
    
    # Optimized class weights for fast precision
    fraud_count = sum(1 for l in train_dataset.labels if l == 0)
    non_fraud_count = sum(1 for l in train_dataset.labels if l == 1)
    
    # Balanced weights for precision
    fraud_weight = 4.0   # Moderate fraud emphasis
    non_fraud_weight = 1.0
    
    print(f"âš¡ Fast-Precision Class weights:")
    print(f"   ğŸš¨ Fraud weight: {fraud_weight}x (optimized)")
    print(f"   âœ… Non-fraud weight: {non_fraud_weight}x")
    print("   âš¡ Optimized for fast precision convergence")
    
    sample_weights = [fraud_weight if l == 0 else non_fraud_weight for l in train_dataset.labels]
    
    # Moderate oversampling for precision
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights) * 3,  # 3x oversampling (optimized)
        replacement=True
    )
    
    train_loader = DataLoader(
        train_dataset, batch_size=12, sampler=sampler,  # Optimized batch size
        num_workers=0, pin_memory=False, drop_last=True
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=12, shuffle=False,
        num_workers=0, pin_memory=False
    )
    
    return train_loader, test_loader

def calculate_fast_metrics(y_true, y_pred):
    """Calculate fast precision metrics"""
    accuracy = accuracy_score(y_true, y_pred)
    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)
    cm = confusion_matrix(y_true, y_pred)
    
    fraud_recall = recall[0] if len(recall) > 0 else 0.0
    fraud_precision = precision[0] if len(precision) > 0 else 0.0
    fraud_f1 = f1[0] if len(f1) > 0 else 0.0
    
    # Fast precision balance score
    fast_balance = (0.6 * fraud_precision + 0.4 * fraud_recall)
    
    return {
        'accuracy': accuracy,
        'fraud_recall': fraud_recall,
        'fraud_precision': fraud_precision,
        'fraud_f1': fraud_f1,
        'fast_balance': fast_balance,
        'confusion_matrix': cm
    }

def fast_precision_training():
    """Fast precision training - 30-45 minutes to 87-88% precision"""
    print("âš¡ Starting Fast-Precision Fraud Detection Training!")
    print("ğŸ¯ Target: 87-88% Fraud Precision with 85%+ Recall")
    print("â±ï¸ Goal: Complete in 30-45 minutes")
    
    # Create optimized model
    model = FastPrecisionDetector().to(device)
    criterion = FastPrecisionFocalLoss(alpha=0.75, gamma=5.0)
    
    # Optimized optimizer for fast convergence
    optimizer = optim.AdamW([
        {'params': model.backbone.parameters(), 'lr': 2e-5, 'weight_decay': 0.01},
        {'params': model.classifier.parameters(), 'lr': 1e-3, 'weight_decay': 0.01}
    ])
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.7, patience=2, min_lr=1e-7
    )
    
    # Create data loaders
    train_loader, test_loader = create_fast_loaders()
    
    print(f"ğŸ“Š Fast precision training setup:")
    print(f"   Train batches: {len(train_loader):,}")
    print(f"   Test batches: {len(test_loader)}")
    
    best_fast_balance = 0.0
    best_model_state = None
    target_precision = 87.0  # 87% precision target
    target_recall = 85.0     # 85% recall minimum
    
    start_time = time.time()
    
    for epoch in range(15):  # Fewer epochs for speed
        epoch_start = time.time()
        print(f"\nâš¡ FAST-PRECISION EPOCH {epoch+1}/15")
        
        # Training phase
        model.train()
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (images, labels) in enumerate(train_loader):
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            epoch_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            if (batch_idx + 1) % 100 == 0:
                acc = 100.0 * correct / total
                avg_loss = epoch_loss / (batch_idx + 1)
                progress = (batch_idx + 1) / len(train_loader) * 100
                print(f"   Progress: {progress:.0f}% | Loss: {avg_loss:.4f} | Acc: {acc:.1f}%")
        
        # Evaluation phase
        model.eval()
        all_preds = []
        all_labels = []
        val_loss = 0.0
        
        with torch.no_grad():
            for images, labels in test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        avg_val_loss = val_loss / len(test_loader)
        metrics = calculate_fast_metrics(all_labels, all_preds)
        
        fraud_recall = metrics['fraud_recall'] * 100
        fraud_precision = metrics['fraud_precision'] * 100
        fraud_f1 = metrics['fraud_f1'] * 100
        accuracy = metrics['accuracy'] * 100
        fast_balance = metrics['fast_balance'] * 100
        
        epoch_time = time.time() - epoch_start
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š EPOCH {epoch+1} FAST-PRECISION RESULTS:")
        print(f"   ğŸ¯ Accuracy: {accuracy:.1f}%")
        print(f"   ğŸš¨ Fraud Recall: {fraud_recall:.1f}%")
        print(f"   ğŸ¯ Fraud Precision: {fraud_precision:.1f}%")
        print(f"   ğŸ“Š Fraud F1-Score: {fraud_f1:.1f}%")
        print(f"   âš¡ Fast-Balance Score: {fast_balance:.1f}%")
        print(f"   â±ï¸ Epoch time: {epoch_time:.1f}s | Total: {total_time/60:.1f}min")
        print(f"   ğŸ“‹ Confusion Matrix:")
        print(f"      {metrics['confusion_matrix']}")
        
        # Save best model based on fast-balance score
        if fast_balance > best_fast_balance and fraud_precision >= 75:
            best_fast_balance = fast_balance
            best_model_state = model.state_dict().copy()
            torch.save(model, 'fast_precision_fraud_model.pth')
            print(f"   âœ… NEW BEST Fast-Balance: {fast_balance:.1f}% - Model saved!")
        
        # Success check
        if fraud_precision >= target_precision and fraud_recall >= target_recall:
            print(f"\nğŸ‰ FAST-PRECISION TARGET ACHIEVED!")
            print(f"   ğŸ¯ Precision: {fraud_precision:.1f}% (â‰¥{target_precision}%)")
            print(f"   ğŸš¨ Recall: {fraud_recall:.1f}% (â‰¥{target_recall}%)")
            print(f"   â±ï¸ Completed in: {total_time/60:.1f} minutes!")
            break
        
        # Learning rate scheduling
        scheduler.step(fast_balance)
        
        print("="*60)
    
    total_training_time = time.time() - start_time
    print(f"\nğŸ FAST-PRECISION TRAINING COMPLETE!")
    print(f"âš¡ Best Fast-Balance Score: {best_fast_balance:.1f}%")
    print(f"â±ï¸ Total training time: {total_training_time/60:.1f} minutes")
    
    # Load best model for final evaluation
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    # Final comprehensive test
    model.eval()
    final_preds = []
    final_labels = []
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            
            final_preds.extend(predicted.cpu().numpy())
            final_labels.extend(labels.cpu().numpy())
    
    final_metrics = calculate_fast_metrics(final_labels, final_preds)
    
    print(f"\nâš¡ FINAL FAST-PRECISION MODEL METRICS:")
    print(f"   ğŸ“Š Overall Accuracy: {final_metrics['accuracy']*100:.1f}%")
    print(f"   ğŸš¨ Fraud Recall: {final_metrics['fraud_recall']*100:.1f}%")
    print(f"   ğŸ¯ Fraud Precision: {final_metrics['fraud_precision']*100:.1f}%")
    print(f"   ğŸ“Š Fraud F1-Score: {final_metrics['fraud_f1']*100:.1f}%")
    print(f"   âš¡ Fast-Balance Score: {final_metrics['fast_balance']*100:.1f}%")
    print(f"   ğŸ“‹ Final Confusion Matrix:")
    print(f"      {final_metrics['confusion_matrix']}")
    
    fraud_recall_final = final_metrics['fraud_recall'] * 100
    fraud_precision_final = final_metrics['fraud_precision'] * 100
    
    if fraud_precision_final >= 87 and fraud_recall_final >= 85:
        print("\nğŸ‰ FAST-PRECISION TARGET ACHIEVED!")
        print("âš¡ Model ready for high-precision deployment!")
    elif fraud_precision_final >= 85 and fraud_recall_final >= 80:
        print("\nâœ… EXCELLENT FAST-PRECISION! Great performance achieved quickly.")
    elif fraud_precision_final >= 80:
        print("\nğŸ“ˆ GOOD FAST-PRECISION! Consider one more training run.")
    else:
        print("\nâš ï¸ Consider adjusting parameters for better precision.")

if __name__ == "__main__":
    fast_precision_training()